{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Activation, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "import keras.optimizers as optimizers\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend import tensorflow_backend\n",
    "\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "session = tf.Session(config=config)\n",
    "tensorflow_backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width, img_height = 200, 200\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 300\n",
    "STEPS = 10\n",
    "CATEGORY = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_names = [name for name in os.listdir(train_data_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disgust',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'surprised',\n",
       " 'neutral',\n",
       " 'angry',\n",
       " 'contempt',\n",
       " 'fear']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデルを作る\n",
    "def inceptionV3fc_model():\n",
    "    input_tensor = Input(shape=(img_height, img_width, 3))\n",
    "    inceptionv3 = InceptionV3(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "    \n",
    "    # 全結合１\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=inceptionv3.output_shape[1:]))\n",
    "    top_model.add(Dense(512, name=\"dense1\"))\n",
    "    top_model.add(Activation(\"relu\"))\n",
    "    top_model.add(Dropout(0.5))\n",
    "        \n",
    "    # 出力\n",
    "    top_model.add(Dense(8, name='output'))\n",
    "    top_model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    # 接続\n",
    "    model = Model(inputs=inceptionv3.input, outputs=top_model(inceptionv3.output))\n",
    "    \n",
    "    # 元の特徴抽出部分は更新しない\n",
    "#    for layer in model.layers[:249]:\n",
    "#        layer.trainable = False\n",
    "#    for layer in model.layers[249:]:\n",
    "#        layer.trainable = True\n",
    "\n",
    "    for layer in model.layers[:241]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[241:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "        \n",
    "#    for i, layer in enumerate(model.layers):\n",
    "#        print(str(i) + \": \" + layer.name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = inceptionV3fc_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 200, 200, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 99, 99, 32)    864         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 99, 99, 32)    96          conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 99, 99, 32)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 97, 97, 32)    9216        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 97, 97, 32)    96          conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 97, 97, 32)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 97, 97, 64)    18432       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 97, 97, 64)    192         conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 97, 97, 64)    0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 48, 48, 64)    0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 48, 48, 80)    5120        max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 48, 48, 80)    240         conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 48, 48, 80)    0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 46, 46, 192)   138240      activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 46, 46, 192)   576         conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 46, 46, 192)   0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 22, 22, 192)   0           activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 22, 22, 64)    12288       max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 22, 22, 64)    192         conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 22, 22, 64)    0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 22, 22, 48)    9216        max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 22, 22, 96)    55296       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 22, 22, 48)    144         conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 22, 22, 96)    288         conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 22, 22, 48)    0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 22, 22, 96)    0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePool (None, 22, 22, 192)   0           max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 22, 22, 64)    12288       max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 22, 22, 64)    76800       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 22, 22, 96)    82944       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 22, 22, 32)    6144        average_pooling2d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 22, 22, 64)    192         conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 22, 22, 64)    192         conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 22, 22, 96)    288         conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 22, 22, 32)    96          conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 22, 22, 64)    0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 22, 22, 64)    0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 22, 22, 96)    0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 22, 22, 32)    0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)             (None, 22, 22, 256)   0           activation_6[0][0]               \n",
      "                                                                   activation_8[0][0]               \n",
      "                                                                   activation_11[0][0]              \n",
      "                                                                   activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, 22, 22, 64)    16384       mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 22, 22, 64)    192         conv2d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 22, 22, 64)    0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 22, 22, 48)    12288       mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)               (None, 22, 22, 96)    55296       activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 22, 22, 48)    144         conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 22, 22, 96)    288         conv2d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 22, 22, 48)    0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 22, 22, 96)    0           batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePool (None, 22, 22, 256)   0           mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 22, 22, 64)    16384       mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 22, 22, 64)    76800       activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)               (None, 22, 22, 96)    82944       activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)               (None, 22, 22, 64)    16384       average_pooling2d_2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 22, 22, 64)    192         conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 22, 22, 64)    192         conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, 22, 22, 96)    288         conv2d_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, 22, 22, 64)    192         conv2d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 22, 22, 64)    0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 22, 22, 64)    0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 22, 22, 96)    0           batch_normalization_18[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 22, 22, 64)    0           batch_normalization_19[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)             (None, 22, 22, 288)   0           activation_13[0][0]              \n",
      "                                                                   activation_15[0][0]              \n",
      "                                                                   activation_18[0][0]              \n",
      "                                                                   activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)               (None, 22, 22, 64)    18432       mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNor (None, 22, 22, 64)    192         conv2d_23[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 22, 22, 64)    0           batch_normalization_23[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)               (None, 22, 22, 48)    13824       mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)               (None, 22, 22, 96)    55296       activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNor (None, 22, 22, 48)    144         conv2d_21[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNor (None, 22, 22, 96)    288         conv2d_24[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 22, 22, 48)    0           batch_normalization_21[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 22, 22, 96)    0           batch_normalization_24[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePool (None, 22, 22, 288)   0           mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)               (None, 22, 22, 64)    18432       mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)               (None, 22, 22, 64)    76800       activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)               (None, 22, 22, 96)    82944       activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)               (None, 22, 22, 64)    18432       average_pooling2d_3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNor (None, 22, 22, 64)    192         conv2d_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNor (None, 22, 22, 64)    192         conv2d_22[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNor (None, 22, 22, 96)    288         conv2d_25[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNor (None, 22, 22, 64)    192         conv2d_26[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 22, 22, 64)    0           batch_normalization_20[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 22, 22, 64)    0           batch_normalization_22[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 22, 22, 96)    0           batch_normalization_25[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 22, 22, 64)    0           batch_normalization_26[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)             (None, 22, 22, 288)   0           activation_20[0][0]              \n",
      "                                                                   activation_22[0][0]              \n",
      "                                                                   activation_25[0][0]              \n",
      "                                                                   activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)               (None, 22, 22, 64)    18432       mixed2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNor (None, 22, 22, 64)    192         conv2d_28[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, 22, 22, 64)    0           batch_normalization_28[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)               (None, 22, 22, 96)    55296       activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNor (None, 22, 22, 96)    288         conv2d_29[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, 22, 22, 96)    0           batch_normalization_29[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)               (None, 10, 10, 384)   995328      mixed2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)               (None, 10, 10, 96)    82944       activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNor (None, 10, 10, 384)   1152        conv2d_27[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNor (None, 10, 10, 96)    288         conv2d_30[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 10, 10, 384)   0           batch_normalization_27[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 10, 10, 96)    0           batch_normalization_30[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 10, 10, 288)   0           mixed2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)             (None, 10, 10, 768)   0           activation_27[0][0]              \n",
      "                                                                   activation_30[0][0]              \n",
      "                                                                   max_pooling2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)               (None, 10, 10, 128)   98304       mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNor (None, 10, 10, 128)   384         conv2d_35[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 10, 10, 128)   0           batch_normalization_35[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)               (None, 10, 10, 128)   114688      activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNor (None, 10, 10, 128)   384         conv2d_36[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 10, 10, 128)   0           batch_normalization_36[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)               (None, 10, 10, 128)   98304       mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)               (None, 10, 10, 128)   114688      activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNor (None, 10, 10, 128)   384         conv2d_32[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNor (None, 10, 10, 128)   384         conv2d_37[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 10, 10, 128)   0           batch_normalization_32[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, 10, 10, 128)   0           batch_normalization_37[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)               (None, 10, 10, 128)   114688      activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)               (None, 10, 10, 128)   114688      activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNor (None, 10, 10, 128)   384         conv2d_33[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNor (None, 10, 10, 128)   384         conv2d_38[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, 10, 10, 128)   0           batch_normalization_33[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, 10, 10, 128)   0           batch_normalization_38[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePool (None, 10, 10, 768)   0           mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)               (None, 10, 10, 192)   147456      mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)               (None, 10, 10, 192)   172032      activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)               (None, 10, 10, 192)   172032      activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)               (None, 10, 10, 192)   147456      average_pooling2d_4[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNor (None, 10, 10, 192)   576         conv2d_31[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNor (None, 10, 10, 192)   576         conv2d_34[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNor (None, 10, 10, 192)   576         conv2d_39[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNor (None, 10, 10, 192)   576         conv2d_40[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 10, 10, 192)   0           batch_normalization_31[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 10, 10, 192)   0           batch_normalization_34[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, 10, 10, 192)   0           batch_normalization_39[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_40 (Activation)       (None, 10, 10, 192)   0           batch_normalization_40[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)             (None, 10, 10, 768)   0           activation_31[0][0]              \n",
      "                                                                   activation_34[0][0]              \n",
      "                                                                   activation_39[0][0]              \n",
      "                                                                   activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)               (None, 10, 10, 160)   122880      mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNor (None, 10, 10, 160)   480         conv2d_45[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_45 (Activation)       (None, 10, 10, 160)   0           batch_normalization_45[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)               (None, 10, 10, 160)   179200      activation_45[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNor (None, 10, 10, 160)   480         conv2d_46[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_46 (Activation)       (None, 10, 10, 160)   0           batch_normalization_46[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)               (None, 10, 10, 160)   122880      mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)               (None, 10, 10, 160)   179200      activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNor (None, 10, 10, 160)   480         conv2d_42[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNor (None, 10, 10, 160)   480         conv2d_47[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_42 (Activation)       (None, 10, 10, 160)   0           batch_normalization_42[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, 10, 10, 160)   0           batch_normalization_47[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)               (None, 10, 10, 160)   179200      activation_42[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)               (None, 10, 10, 160)   179200      activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNor (None, 10, 10, 160)   480         conv2d_43[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNor (None, 10, 10, 160)   480         conv2d_48[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_43 (Activation)       (None, 10, 10, 160)   0           batch_normalization_43[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, 10, 10, 160)   0           batch_normalization_48[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePool (None, 10, 10, 768)   0           mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)               (None, 10, 10, 192)   147456      mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)               (None, 10, 10, 192)   215040      activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)               (None, 10, 10, 192)   215040      activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)               (None, 10, 10, 192)   147456      average_pooling2d_5[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNor (None, 10, 10, 192)   576         conv2d_41[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNor (None, 10, 10, 192)   576         conv2d_44[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNor (None, 10, 10, 192)   576         conv2d_49[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNor (None, 10, 10, 192)   576         conv2d_50[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_41 (Activation)       (None, 10, 10, 192)   0           batch_normalization_41[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_44 (Activation)       (None, 10, 10, 192)   0           batch_normalization_44[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, 10, 10, 192)   0           batch_normalization_49[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_50 (Activation)       (None, 10, 10, 192)   0           batch_normalization_50[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)             (None, 10, 10, 768)   0           activation_41[0][0]              \n",
      "                                                                   activation_44[0][0]              \n",
      "                                                                   activation_49[0][0]              \n",
      "                                                                   activation_50[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)               (None, 10, 10, 160)   122880      mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNor (None, 10, 10, 160)   480         conv2d_55[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_55 (Activation)       (None, 10, 10, 160)   0           batch_normalization_55[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)               (None, 10, 10, 160)   179200      activation_55[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNor (None, 10, 10, 160)   480         conv2d_56[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_56 (Activation)       (None, 10, 10, 160)   0           batch_normalization_56[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)               (None, 10, 10, 160)   122880      mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)               (None, 10, 10, 160)   179200      activation_56[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNor (None, 10, 10, 160)   480         conv2d_52[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNor (None, 10, 10, 160)   480         conv2d_57[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_52 (Activation)       (None, 10, 10, 160)   0           batch_normalization_52[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_57 (Activation)       (None, 10, 10, 160)   0           batch_normalization_57[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)               (None, 10, 10, 160)   179200      activation_52[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)               (None, 10, 10, 160)   179200      activation_57[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNor (None, 10, 10, 160)   480         conv2d_53[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNor (None, 10, 10, 160)   480         conv2d_58[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_53 (Activation)       (None, 10, 10, 160)   0           batch_normalization_53[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_58 (Activation)       (None, 10, 10, 160)   0           batch_normalization_58[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePool (None, 10, 10, 768)   0           mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)               (None, 10, 10, 192)   147456      mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)               (None, 10, 10, 192)   215040      activation_53[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)               (None, 10, 10, 192)   215040      activation_58[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)               (None, 10, 10, 192)   147456      average_pooling2d_6[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNor (None, 10, 10, 192)   576         conv2d_51[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNor (None, 10, 10, 192)   576         conv2d_54[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNor (None, 10, 10, 192)   576         conv2d_59[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNor (None, 10, 10, 192)   576         conv2d_60[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_51 (Activation)       (None, 10, 10, 192)   0           batch_normalization_51[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_54 (Activation)       (None, 10, 10, 192)   0           batch_normalization_54[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_59 (Activation)       (None, 10, 10, 192)   0           batch_normalization_59[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_60 (Activation)       (None, 10, 10, 192)   0           batch_normalization_60[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)             (None, 10, 10, 768)   0           activation_51[0][0]              \n",
      "                                                                   activation_54[0][0]              \n",
      "                                                                   activation_59[0][0]              \n",
      "                                                                   activation_60[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)               (None, 10, 10, 192)   147456      mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNor (None, 10, 10, 192)   576         conv2d_65[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_65 (Activation)       (None, 10, 10, 192)   0           batch_normalization_65[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)               (None, 10, 10, 192)   258048      activation_65[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNor (None, 10, 10, 192)   576         conv2d_66[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_66 (Activation)       (None, 10, 10, 192)   0           batch_normalization_66[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)               (None, 10, 10, 192)   147456      mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)               (None, 10, 10, 192)   258048      activation_66[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNor (None, 10, 10, 192)   576         conv2d_62[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNor (None, 10, 10, 192)   576         conv2d_67[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_62 (Activation)       (None, 10, 10, 192)   0           batch_normalization_62[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_67 (Activation)       (None, 10, 10, 192)   0           batch_normalization_67[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)               (None, 10, 10, 192)   258048      activation_62[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)               (None, 10, 10, 192)   258048      activation_67[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNor (None, 10, 10, 192)   576         conv2d_63[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNor (None, 10, 10, 192)   576         conv2d_68[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_63 (Activation)       (None, 10, 10, 192)   0           batch_normalization_63[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_68 (Activation)       (None, 10, 10, 192)   0           batch_normalization_68[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePool (None, 10, 10, 768)   0           mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)               (None, 10, 10, 192)   147456      mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)               (None, 10, 10, 192)   258048      activation_63[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)               (None, 10, 10, 192)   258048      activation_68[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)               (None, 10, 10, 192)   147456      average_pooling2d_7[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNor (None, 10, 10, 192)   576         conv2d_61[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNor (None, 10, 10, 192)   576         conv2d_64[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNor (None, 10, 10, 192)   576         conv2d_69[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNor (None, 10, 10, 192)   576         conv2d_70[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_61 (Activation)       (None, 10, 10, 192)   0           batch_normalization_61[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_64 (Activation)       (None, 10, 10, 192)   0           batch_normalization_64[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_69 (Activation)       (None, 10, 10, 192)   0           batch_normalization_69[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_70 (Activation)       (None, 10, 10, 192)   0           batch_normalization_70[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)             (None, 10, 10, 768)   0           activation_61[0][0]              \n",
      "                                                                   activation_64[0][0]              \n",
      "                                                                   activation_69[0][0]              \n",
      "                                                                   activation_70[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)               (None, 10, 10, 192)   147456      mixed7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNor (None, 10, 10, 192)   576         conv2d_73[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_73 (Activation)       (None, 10, 10, 192)   0           batch_normalization_73[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)               (None, 10, 10, 192)   258048      activation_73[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNor (None, 10, 10, 192)   576         conv2d_74[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_74 (Activation)       (None, 10, 10, 192)   0           batch_normalization_74[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)               (None, 10, 10, 192)   147456      mixed7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)               (None, 10, 10, 192)   258048      activation_74[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNor (None, 10, 10, 192)   576         conv2d_71[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNor (None, 10, 10, 192)   576         conv2d_75[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_71 (Activation)       (None, 10, 10, 192)   0           batch_normalization_71[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_75 (Activation)       (None, 10, 10, 192)   0           batch_normalization_75[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)               (None, 4, 4, 320)     552960      activation_71[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)               (None, 4, 4, 192)     331776      activation_75[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNor (None, 4, 4, 320)     960         conv2d_72[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNor (None, 4, 4, 192)     576         conv2d_76[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_72 (Activation)       (None, 4, 4, 320)     0           batch_normalization_72[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_76 (Activation)       (None, 4, 4, 192)     0           batch_normalization_76[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, 4, 4, 768)     0           mixed7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)             (None, 4, 4, 1280)    0           activation_72[0][0]              \n",
      "                                                                   activation_76[0][0]              \n",
      "                                                                   max_pooling2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)               (None, 4, 4, 448)     573440      mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNor (None, 4, 4, 448)     1344        conv2d_81[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_81 (Activation)       (None, 4, 4, 448)     0           batch_normalization_81[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)               (None, 4, 4, 384)     491520      mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)               (None, 4, 4, 384)     1548288     activation_81[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNor (None, 4, 4, 384)     1152        conv2d_78[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNor (None, 4, 4, 384)     1152        conv2d_82[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_78 (Activation)       (None, 4, 4, 384)     0           batch_normalization_78[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_82 (Activation)       (None, 4, 4, 384)     0           batch_normalization_82[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)               (None, 4, 4, 384)     442368      activation_78[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)               (None, 4, 4, 384)     442368      activation_78[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)               (None, 4, 4, 384)     442368      activation_82[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)               (None, 4, 4, 384)     442368      activation_82[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePool (None, 4, 4, 1280)    0           mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)               (None, 4, 4, 320)     409600      mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNor (None, 4, 4, 384)     1152        conv2d_79[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNor (None, 4, 4, 384)     1152        conv2d_80[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNor (None, 4, 4, 384)     1152        conv2d_83[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNor (None, 4, 4, 384)     1152        conv2d_84[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)               (None, 4, 4, 192)     245760      average_pooling2d_8[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNor (None, 4, 4, 320)     960         conv2d_77[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_79 (Activation)       (None, 4, 4, 384)     0           batch_normalization_79[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_80 (Activation)       (None, 4, 4, 384)     0           batch_normalization_80[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_83 (Activation)       (None, 4, 4, 384)     0           batch_normalization_83[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_84 (Activation)       (None, 4, 4, 384)     0           batch_normalization_84[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNor (None, 4, 4, 192)     576         conv2d_85[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_77 (Activation)       (None, 4, 4, 320)     0           batch_normalization_77[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)           (None, 4, 4, 768)     0           activation_79[0][0]              \n",
      "                                                                   activation_80[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 4, 4, 768)     0           activation_83[0][0]              \n",
      "                                                                   activation_84[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_85 (Activation)       (None, 4, 4, 192)     0           batch_normalization_85[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)             (None, 4, 4, 2048)    0           activation_77[0][0]              \n",
      "                                                                   mixed9_0[0][0]                   \n",
      "                                                                   concatenate_1[0][0]              \n",
      "                                                                   activation_85[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)               (None, 4, 4, 448)     917504      mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNor (None, 4, 4, 448)     1344        conv2d_90[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_90 (Activation)       (None, 4, 4, 448)     0           batch_normalization_90[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)               (None, 4, 4, 384)     786432      mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)               (None, 4, 4, 384)     1548288     activation_90[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNor (None, 4, 4, 384)     1152        conv2d_87[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNor (None, 4, 4, 384)     1152        conv2d_91[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_87 (Activation)       (None, 4, 4, 384)     0           batch_normalization_87[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_91 (Activation)       (None, 4, 4, 384)     0           batch_normalization_91[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)               (None, 4, 4, 384)     442368      activation_87[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)               (None, 4, 4, 384)     442368      activation_87[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)               (None, 4, 4, 384)     442368      activation_91[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)               (None, 4, 4, 384)     442368      activation_91[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePool (None, 4, 4, 2048)    0           mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)               (None, 4, 4, 320)     655360      mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNor (None, 4, 4, 384)     1152        conv2d_88[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNor (None, 4, 4, 384)     1152        conv2d_89[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNor (None, 4, 4, 384)     1152        conv2d_92[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNor (None, 4, 4, 384)     1152        conv2d_93[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)               (None, 4, 4, 192)     393216      average_pooling2d_9[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNor (None, 4, 4, 320)     960         conv2d_86[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_88 (Activation)       (None, 4, 4, 384)     0           batch_normalization_88[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_89 (Activation)       (None, 4, 4, 384)     0           batch_normalization_89[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_92 (Activation)       (None, 4, 4, 384)     0           batch_normalization_92[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_93 (Activation)       (None, 4, 4, 384)     0           batch_normalization_93[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNor (None, 4, 4, 192)     576         conv2d_94[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_86 (Activation)       (None, 4, 4, 320)     0           batch_normalization_86[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)           (None, 4, 4, 768)     0           activation_88[0][0]              \n",
      "                                                                   activation_89[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 4, 4, 768)     0           activation_92[0][0]              \n",
      "                                                                   activation_93[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_94 (Activation)       (None, 4, 4, 192)     0           batch_normalization_94[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)            (None, 4, 4, 2048)    0           activation_86[0][0]              \n",
      "                                                                   mixed9_1[0][0]                   \n",
      "                                                                   concatenate_2[0][0]              \n",
      "                                                                   activation_94[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)        (None, 8)             16781832    mixed10[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 38,584,616\n",
      "Trainable params: 28,781,960\n",
      "Non-trainable params: 9,802,656\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              # optimizer='rmsprop',\n",
    "              optimizer=optimizers.Adam(lr=0.0001),\n",
    "#              optimizer=optimizers.SGD(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=60,\n",
    "#    zca_whitening=True,\n",
    "#    samplewise_center=True,\n",
    "#    featurewise_center=True,\n",
    "    width_shift_range=0.1,\n",
    "#    height_shift_range=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 561 images belonging to 8 classes.\n",
      "Found 160 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=BATCH_SIZE,\n",
    "#    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    classes=category_names,\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=20,\n",
    "    classes=category_names,\n",
    "#    color_mode=\"grayscale\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "10/10 [==============================] - 9s - loss: 2.5764 - acc: 0.1712     \n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 4s - loss: 2.1234 - acc: 0.2465     \n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 4s - loss: 1.8387 - acc: 0.2895     \n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 4s - loss: 1.7593 - acc: 0.3327     \n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 4s - loss: 1.5662 - acc: 0.4037     \n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 4s - loss: 1.6031 - acc: 0.4174     \n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 4s - loss: 1.4500 - acc: 0.4704     \n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 5s - loss: 1.3375 - acc: 0.5189     \n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 4s - loss: 1.3510 - acc: 0.5043     \n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 4s - loss: 1.2285 - acc: 0.5379     \n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 4s - loss: 1.2025 - acc: 0.5660     \n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 5s - loss: 1.1336 - acc: 0.6062     \n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 5s - loss: 1.1727 - acc: 0.5896     \n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 4s - loss: 1.0535 - acc: 0.6358     \n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 5s - loss: 0.9193 - acc: 0.6821     \n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 4s - loss: 0.8969 - acc: 0.6909     \n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 5s - loss: 0.8845 - acc: 0.7020     \n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 4s - loss: 0.8008 - acc: 0.7304     \n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 4s - loss: 0.7265 - acc: 0.7321     \n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 5s - loss: 0.6352 - acc: 0.7901     \n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 4s - loss: 0.8098 - acc: 0.7199     \n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 4s - loss: 0.6290 - acc: 0.7801     \n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 4s - loss: 0.6190 - acc: 0.7683     \n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 4s - loss: 0.6049 - acc: 0.7939     \n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 4s - loss: 0.6365 - acc: 0.7825     \n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 4s - loss: 0.5247 - acc: 0.8218     \n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 4s - loss: 0.5854 - acc: 0.7827     \n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 4s - loss: 0.4740 - acc: 0.8464     \n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 4s - loss: 0.4451 - acc: 0.8599     \n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 5s - loss: 0.4385 - acc: 0.8401     \n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 5s - loss: 0.3938 - acc: 0.8612     \n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 4s - loss: 0.4254 - acc: 0.8576     \n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 4s - loss: 0.4182 - acc: 0.8406     \n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 5s - loss: 0.4633 - acc: 0.8404     \n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 5s - loss: 0.4170 - acc: 0.8918     \n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 4s - loss: 0.3559 - acc: 0.8775     \n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 4s - loss: 0.3421 - acc: 0.8826     \n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 5s - loss: 0.3465 - acc: 0.8819     \n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 5s - loss: 0.3721 - acc: 0.8761     \n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 5s - loss: 0.3445 - acc: 0.8819     \n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 4s - loss: 0.2834 - acc: 0.9081     \n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 4s - loss: 0.3007 - acc: 0.8929     \n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 4s - loss: 0.2663 - acc: 0.9106     \n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 4s - loss: 0.2774 - acc: 0.9166     \n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 4s - loss: 0.2904 - acc: 0.9075     \n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 5s - loss: 0.2455 - acc: 0.9173     \n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 4s - loss: 0.2352 - acc: 0.9242     \n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 4s - loss: 0.2906 - acc: 0.8956     \n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 4s - loss: 0.2717 - acc: 0.9195     \n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 4s - loss: 0.2400 - acc: 0.9218     \n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 5s - loss: 0.2734 - acc: 0.9160     \n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 4s - loss: 0.2128 - acc: 0.9341     \n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 4s - loss: 0.2555 - acc: 0.9086     \n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 4s - loss: 0.1838 - acc: 0.9389     \n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 4s - loss: 0.2126 - acc: 0.9202     \n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 5s - loss: 0.1788 - acc: 0.9392     \n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 5s - loss: 0.1711 - acc: 0.9463     \n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 5s - loss: 0.1829 - acc: 0.9341     \n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 5s - loss: 0.1858 - acc: 0.9322     \n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 4s - loss: 0.1456 - acc: 0.9490     \n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 4s - loss: 0.1396 - acc: 0.9470     \n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 4s - loss: 0.2192 - acc: 0.9291     \n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 4s - loss: 0.1730 - acc: 0.9432     \n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 5s - loss: 0.1387 - acc: 0.9575     \n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 5s - loss: 0.1744 - acc: 0.9381     \n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 5s - loss: 0.1887 - acc: 0.9385     \n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 5s - loss: 0.1527 - acc: 0.9412     \n",
      "Epoch 68/300\n",
      "10/10 [==============================] - 4s - loss: 0.1457 - acc: 0.9517     \n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 5s - loss: 0.1365 - acc: 0.9506     \n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 5s - loss: 0.1617 - acc: 0.9553     \n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 4s - loss: 0.0925 - acc: 0.9678     \n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 4s - loss: 0.1112 - acc: 0.9700     \n",
      "Epoch 73/300\n",
      "10/10 [==============================] - 4s - loss: 0.0913 - acc: 0.9718     \n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 5s - loss: 0.1083 - acc: 0.9682     \n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 5s - loss: 0.1349 - acc: 0.9495     \n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 5s - loss: 0.1220 - acc: 0.9669     \n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 5s - loss: 0.1401 - acc: 0.9502     \n",
      "Epoch 78/300\n",
      "10/10 [==============================] - 5s - loss: 0.1434 - acc: 0.9452     \n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 4s - loss: 0.1556 - acc: 0.9564     \n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 4s - loss: 0.1627 - acc: 0.9549     \n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 4s - loss: 0.0992 - acc: 0.9586     \n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 5s - loss: 0.1224 - acc: 0.9589     \n",
      "Epoch 83/300\n",
      "10/10 [==============================] - 4s - loss: 0.1211 - acc: 0.9560     \n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 5s - loss: 0.0965 - acc: 0.9654     \n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 5s - loss: 0.1816 - acc: 0.9497     \n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 5s - loss: 0.0925 - acc: 0.9642     \n",
      "Epoch 87/300\n",
      "10/10 [==============================] - 4s - loss: 0.1479 - acc: 0.9620     \n",
      "Epoch 88/300\n",
      "10/10 [==============================] - 5s - loss: 0.0720 - acc: 0.9749     \n",
      "Epoch 89/300\n",
      "10/10 [==============================] - 5s - loss: 0.1436 - acc: 0.9506     \n",
      "Epoch 90/300\n",
      "10/10 [==============================] - 4s - loss: 0.1036 - acc: 0.9595     \n",
      "Epoch 91/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 4s - loss: 0.0902 - acc: 0.9694     \n",
      "Epoch 92/300\n",
      "10/10 [==============================] - 4s - loss: 0.1327 - acc: 0.9635     \n",
      "Epoch 93/300\n",
      "10/10 [==============================] - 4s - loss: 0.1290 - acc: 0.9580     \n",
      "Epoch 94/300\n",
      "10/10 [==============================] - 5s - loss: 0.1402 - acc: 0.9557     \n",
      "Epoch 95/300\n",
      "10/10 [==============================] - 5s - loss: 0.0836 - acc: 0.9698     \n",
      "Epoch 96/300\n",
      "10/10 [==============================] - 5s - loss: 0.1055 - acc: 0.9678     \n",
      "Epoch 97/300\n",
      "10/10 [==============================] - 4s - loss: 0.1308 - acc: 0.9573     \n",
      "Epoch 98/300\n",
      "10/10 [==============================] - 5s - loss: 0.1181 - acc: 0.9651     \n",
      "Epoch 99/300\n",
      "10/10 [==============================] - 4s - loss: 0.0941 - acc: 0.9624     \n",
      "Epoch 100/300\n",
      "10/10 [==============================] - 4s - loss: 0.1454 - acc: 0.9593     \n",
      "Epoch 101/300\n",
      "10/10 [==============================] - 5s - loss: 0.0868 - acc: 0.9662     \n",
      "Epoch 102/300\n",
      "10/10 [==============================] - 5s - loss: 0.0985 - acc: 0.9661     \n",
      "Epoch 103/300\n",
      "10/10 [==============================] - 4s - loss: 0.1152 - acc: 0.9595     \n",
      "Epoch 104/300\n",
      "10/10 [==============================] - 4s - loss: 0.1214 - acc: 0.9667     \n",
      "Epoch 105/300\n",
      "10/10 [==============================] - 5s - loss: 0.0739 - acc: 0.9765     \n",
      "Epoch 106/300\n",
      "10/10 [==============================] - 4s - loss: 0.1062 - acc: 0.9618     \n",
      "Epoch 107/300\n",
      "10/10 [==============================] - 4s - loss: 0.0747 - acc: 0.9761     \n",
      "Epoch 108/300\n",
      "10/10 [==============================] - 4s - loss: 0.0774 - acc: 0.9752     \n",
      "Epoch 109/300\n",
      "10/10 [==============================] - 5s - loss: 0.0390 - acc: 0.9859     \n",
      "Epoch 110/300\n",
      "10/10 [==============================] - 5s - loss: 0.0724 - acc: 0.9745     \n",
      "Epoch 111/300\n",
      "10/10 [==============================] - 4s - loss: 0.0988 - acc: 0.9662     \n",
      "Epoch 112/300\n",
      "10/10 [==============================] - 5s - loss: 0.1100 - acc: 0.9595     \n",
      "Epoch 113/300\n",
      "10/10 [==============================] - 5s - loss: 0.1105 - acc: 0.9761     \n",
      "Epoch 114/300\n",
      "10/10 [==============================] - 4s - loss: 0.0625 - acc: 0.9752     \n",
      "Epoch 115/300\n",
      "10/10 [==============================] - 5s - loss: 0.0920 - acc: 0.9712     \n",
      "Epoch 116/300\n",
      "10/10 [==============================] - 4s - loss: 0.1002 - acc: 0.9662     \n",
      "Epoch 117/300\n",
      "10/10 [==============================] - 5s - loss: 0.0473 - acc: 0.9906     \n",
      "Epoch 118/300\n",
      "10/10 [==============================] - 5s - loss: 0.0847 - acc: 0.9674     \n",
      "Epoch 119/300\n",
      "10/10 [==============================] - 4s - loss: 0.0560 - acc: 0.9796     \n",
      "Epoch 120/300\n",
      "10/10 [==============================] - 5s - loss: 0.1034 - acc: 0.9658     \n",
      "Epoch 121/300\n",
      "10/10 [==============================] - 5s - loss: 0.0902 - acc: 0.9651     \n",
      "Epoch 122/300\n",
      "10/10 [==============================] - 5s - loss: 0.0632 - acc: 0.9839     \n",
      "Epoch 123/300\n",
      "10/10 [==============================] - 4s - loss: 0.0679 - acc: 0.9761     \n",
      "Epoch 124/300\n",
      "10/10 [==============================] - 5s - loss: 0.1011 - acc: 0.9682     \n",
      "Epoch 125/300\n",
      "10/10 [==============================] - 4s - loss: 0.0651 - acc: 0.9823     \n",
      "Epoch 126/300\n",
      "10/10 [==============================] - 4s - loss: 0.0829 - acc: 0.9680     \n",
      "Epoch 127/300\n",
      "10/10 [==============================] - 4s - loss: 0.0715 - acc: 0.9745     \n",
      "Epoch 128/300\n",
      "10/10 [==============================] - 4s - loss: 0.0533 - acc: 0.9843     \n",
      "Epoch 129/300\n",
      "10/10 [==============================] - 5s - loss: 0.0626 - acc: 0.9734     \n",
      "Epoch 130/300\n",
      "10/10 [==============================] - 4s - loss: 0.0607 - acc: 0.9823     \n",
      "Epoch 131/300\n",
      "10/10 [==============================] - 5s - loss: 0.0695 - acc: 0.9839     \n",
      "Epoch 132/300\n",
      "10/10 [==============================] - 4s - loss: 0.0797 - acc: 0.9716     \n",
      "Epoch 133/300\n",
      "10/10 [==============================] - 5s - loss: 0.0470 - acc: 0.9843     \n",
      "Epoch 134/300\n",
      "10/10 [==============================] - 4s - loss: 0.0791 - acc: 0.9772     \n",
      "Epoch 135/300\n",
      "10/10 [==============================] - 5s - loss: 0.0924 - acc: 0.9716     \n",
      "Epoch 136/300\n",
      "10/10 [==============================] - 4s - loss: 0.0645 - acc: 0.9835     \n",
      "Epoch 137/300\n",
      "10/10 [==============================] - 5s - loss: 0.0440 - acc: 0.9835     \n",
      "Epoch 138/300\n",
      "10/10 [==============================] - 4s - loss: 0.0913 - acc: 0.9761     \n",
      "Epoch 139/300\n",
      "10/10 [==============================] - 5s - loss: 0.0732 - acc: 0.9783     \n",
      "Epoch 140/300\n",
      "10/10 [==============================] - 5s - loss: 0.0453 - acc: 0.9839     \n",
      "Epoch 141/300\n",
      "10/10 [==============================] - 5s - loss: 0.1185 - acc: 0.9712     \n",
      "Epoch 142/300\n",
      "10/10 [==============================] - 4s - loss: 0.0648 - acc: 0.9828     \n",
      "Epoch 143/300\n",
      "10/10 [==============================] - 4s - loss: 0.0704 - acc: 0.9714     \n",
      "Epoch 144/300\n",
      "10/10 [==============================] - 4s - loss: 0.1039 - acc: 0.9704     \n",
      "Epoch 145/300\n",
      "10/10 [==============================] - 5s - loss: 0.0809 - acc: 0.9763     \n",
      "Epoch 146/300\n",
      "10/10 [==============================] - 5s - loss: 0.0794 - acc: 0.9749     \n",
      "Epoch 147/300\n",
      "10/10 [==============================] - 4s - loss: 0.0708 - acc: 0.9749     \n",
      "Epoch 148/300\n",
      "10/10 [==============================] - 4s - loss: 0.0537 - acc: 0.9796     \n",
      "Epoch 149/300\n",
      "10/10 [==============================] - 4s - loss: 0.0501 - acc: 0.9812     \n",
      "Epoch 150/300\n",
      "10/10 [==============================] - 4s - loss: 0.0492 - acc: 0.9792     \n",
      "Epoch 151/300\n",
      "10/10 [==============================] - 4s - loss: 0.0592 - acc: 0.9828     \n",
      "Epoch 152/300\n",
      "10/10 [==============================] - 4s - loss: 0.0431 - acc: 0.9819     \n",
      "Epoch 153/300\n",
      "10/10 [==============================] - 4s - loss: 0.0487 - acc: 0.9874     \n",
      "Epoch 154/300\n",
      "10/10 [==============================] - 5s - loss: 0.0381 - acc: 0.9870     \n",
      "Epoch 155/300\n",
      "10/10 [==============================] - 5s - loss: 0.0511 - acc: 0.9870     \n",
      "Epoch 156/300\n",
      "10/10 [==============================] - 5s - loss: 0.0474 - acc: 0.9839     \n",
      "Epoch 157/300\n",
      "10/10 [==============================] - 4s - loss: 0.0566 - acc: 0.9812     \n",
      "Epoch 158/300\n",
      "10/10 [==============================] - 4s - loss: 0.0562 - acc: 0.9855     \n",
      "Epoch 159/300\n",
      "10/10 [==============================] - 5s - loss: 0.0568 - acc: 0.9803     \n",
      "Epoch 160/300\n",
      "10/10 [==============================] - 5s - loss: 0.0501 - acc: 0.9828     \n",
      "Epoch 161/300\n",
      "10/10 [==============================] - 5s - loss: 0.0456 - acc: 0.9875     \n",
      "Epoch 162/300\n",
      "10/10 [==============================] - 4s - loss: 0.0403 - acc: 0.9906     \n",
      "Epoch 163/300\n",
      "10/10 [==============================] - 4s - loss: 0.0304 - acc: 0.9886     \n",
      "Epoch 164/300\n",
      "10/10 [==============================] - 4s - loss: 0.0619 - acc: 0.9859     \n",
      "Epoch 165/300\n",
      "10/10 [==============================] - 4s - loss: 0.0661 - acc: 0.9768     \n",
      "Epoch 166/300\n",
      "10/10 [==============================] - 5s - loss: 0.0523 - acc: 0.9839     \n",
      "Epoch 167/300\n",
      "10/10 [==============================] - 5s - loss: 0.0891 - acc: 0.9752     \n",
      "Epoch 168/300\n",
      "10/10 [==============================] - 4s - loss: 0.0489 - acc: 0.9819     \n",
      "Epoch 169/300\n",
      "10/10 [==============================] - 4s - loss: 0.0948 - acc: 0.9756     \n",
      "Epoch 170/300\n",
      "10/10 [==============================] - 4s - loss: 0.0950 - acc: 0.9765     \n",
      "Epoch 171/300\n",
      "10/10 [==============================] - 4s - loss: 0.0970 - acc: 0.9803     \n",
      "Epoch 172/300\n",
      "10/10 [==============================] - 4s - loss: 0.0632 - acc: 0.9870     \n",
      "Epoch 173/300\n",
      "10/10 [==============================] - 4s - loss: 0.0630 - acc: 0.9828     \n",
      "Epoch 174/300\n",
      "10/10 [==============================] - 4s - loss: 0.0374 - acc: 0.9886     \n",
      "Epoch 175/300\n",
      "10/10 [==============================] - 4s - loss: 0.0964 - acc: 0.9705     \n",
      "Epoch 176/300\n",
      "10/10 [==============================] - 4s - loss: 0.0478 - acc: 0.9812     \n",
      "Epoch 177/300\n",
      "10/10 [==============================] - 4s - loss: 0.0326 - acc: 0.9828     \n",
      "Epoch 178/300\n",
      "10/10 [==============================] - 4s - loss: 0.0414 - acc: 0.9839     \n",
      "Epoch 179/300\n",
      "10/10 [==============================] - 4s - loss: 0.0826 - acc: 0.9812     \n",
      "Epoch 180/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 4s - loss: 0.0450 - acc: 0.9922     \n",
      "Epoch 181/300\n",
      "10/10 [==============================] - 4s - loss: 0.0409 - acc: 0.9828     \n",
      "Epoch 182/300\n",
      "10/10 [==============================] - 4s - loss: 0.0287 - acc: 0.9890     \n",
      "Epoch 183/300\n",
      "10/10 [==============================] - 4s - loss: 0.0480 - acc: 0.9870     \n",
      "Epoch 184/300\n",
      "10/10 [==============================] - 4s - loss: 0.0492 - acc: 0.9808     \n",
      "Epoch 185/300\n",
      "10/10 [==============================] - 4s - loss: 0.0313 - acc: 0.9913     \n",
      "Epoch 186/300\n",
      "10/10 [==============================] - 5s - loss: 0.0660 - acc: 0.9792     \n",
      "Epoch 187/300\n",
      "10/10 [==============================] - 4s - loss: 0.0394 - acc: 0.9870     \n",
      "Epoch 188/300\n",
      "10/10 [==============================] - 4s - loss: 0.0441 - acc: 0.9835     \n",
      "Epoch 189/300\n",
      "10/10 [==============================] - 4s - loss: 0.0644 - acc: 0.9850     \n",
      "Epoch 190/300\n",
      "10/10 [==============================] - 5s - loss: 0.0741 - acc: 0.9768     \n",
      "Epoch 191/300\n",
      "10/10 [==============================] - 4s - loss: 0.0618 - acc: 0.9843     \n",
      "Epoch 192/300\n",
      "10/10 [==============================] - 5s - loss: 0.0372 - acc: 0.9906     \n",
      "Epoch 193/300\n",
      "10/10 [==============================] - 4s - loss: 0.0592 - acc: 0.9819     \n",
      "Epoch 194/300\n",
      "10/10 [==============================] - 5s - loss: 0.0502 - acc: 0.9788     \n",
      "Epoch 195/300\n",
      "10/10 [==============================] - 4s - loss: 0.0293 - acc: 0.9855     \n",
      "Epoch 196/300\n",
      "10/10 [==============================] - 5s - loss: 0.0536 - acc: 0.9781     \n",
      "Epoch 197/300\n",
      "10/10 [==============================] - 4s - loss: 0.0493 - acc: 0.9855     \n",
      "Epoch 198/300\n",
      "10/10 [==============================] - 5s - loss: 0.0305 - acc: 0.9917     \n",
      "Epoch 199/300\n",
      "10/10 [==============================] - 5s - loss: 0.0510 - acc: 0.9828     \n",
      "Epoch 200/300\n",
      "10/10 [==============================] - 4s - loss: 0.0467 - acc: 0.9866     \n",
      "Epoch 201/300\n",
      "10/10 [==============================] - 5s - loss: 0.0555 - acc: 0.9855     \n",
      "Epoch 202/300\n",
      "10/10 [==============================] - 4s - loss: 0.0402 - acc: 0.9839     \n",
      "Epoch 203/300\n",
      "10/10 [==============================] - 4s - loss: 0.0613 - acc: 0.9823     \n",
      "Epoch 204/300\n",
      "10/10 [==============================] - 5s - loss: 0.0661 - acc: 0.9819     \n",
      "Epoch 205/300\n",
      "10/10 [==============================] - 5s - loss: 0.0233 - acc: 0.9953     \n",
      "Epoch 206/300\n",
      "10/10 [==============================] - 5s - loss: 0.0369 - acc: 0.9875     \n",
      "Epoch 207/300\n",
      "10/10 [==============================] - 4s - loss: 0.0550 - acc: 0.9874     \n",
      "Epoch 208/300\n",
      "10/10 [==============================] - 4s - loss: 0.0424 - acc: 0.9886     \n",
      "Epoch 209/300\n",
      "10/10 [==============================] - 4s - loss: 0.0266 - acc: 0.9933     \n",
      "Epoch 210/300\n",
      "10/10 [==============================] - 4s - loss: 0.0603 - acc: 0.9808     \n",
      "Epoch 211/300\n",
      "10/10 [==============================] - 4s - loss: 0.0544 - acc: 0.9796     \n",
      "Epoch 212/300\n",
      "10/10 [==============================] - 4s - loss: 0.0337 - acc: 0.9839     \n",
      "Epoch 213/300\n",
      "10/10 [==============================] - 4s - loss: 0.0356 - acc: 0.9917     \n",
      "Epoch 214/300\n",
      "10/10 [==============================] - 5s - loss: 0.0349 - acc: 0.9859     \n",
      "Epoch 215/300\n",
      "10/10 [==============================] - 4s - loss: 0.0511 - acc: 0.9828     \n",
      "Epoch 216/300\n",
      "10/10 [==============================] - 4s - loss: 0.0352 - acc: 0.9917     \n",
      "Epoch 217/300\n",
      "10/10 [==============================] - 4s - loss: 0.0606 - acc: 0.9792     \n",
      "Epoch 218/300\n",
      "10/10 [==============================] - 4s - loss: 0.0419 - acc: 0.9870     \n",
      "Epoch 219/300\n",
      "10/10 [==============================] - 4s - loss: 0.0484 - acc: 0.9875     \n",
      "Epoch 220/300\n",
      "10/10 [==============================] - 4s - loss: 0.0375 - acc: 0.9917     \n",
      "Epoch 221/300\n",
      "10/10 [==============================] - 4s - loss: 0.0328 - acc: 0.9886     \n",
      "Epoch 222/300\n",
      "10/10 [==============================] - 4s - loss: 0.0399 - acc: 0.9850     \n",
      "Epoch 223/300\n",
      "10/10 [==============================] - 5s - loss: 0.0387 - acc: 0.9866     \n",
      "Epoch 224/300\n",
      "10/10 [==============================] - 5s - loss: 0.0544 - acc: 0.9855     \n",
      "Epoch 225/300\n",
      "10/10 [==============================] - 4s - loss: 0.0488 - acc: 0.9830     \n",
      "Epoch 226/300\n",
      "10/10 [==============================] - 4s - loss: 0.0399 - acc: 0.9902     \n",
      "Epoch 227/300\n",
      "10/10 [==============================] - 5s - loss: 0.0595 - acc: 0.9835     \n",
      "Epoch 228/300\n",
      "10/10 [==============================] - 5s - loss: 0.0518 - acc: 0.9839     \n",
      "Epoch 229/300\n",
      "10/10 [==============================] - 4s - loss: 0.0445 - acc: 0.9882     \n",
      "Epoch 230/300\n",
      "10/10 [==============================] - 5s - loss: 0.0545 - acc: 0.9866     \n",
      "Epoch 231/300\n",
      "10/10 [==============================] - 5s - loss: 0.0273 - acc: 0.9877     \n",
      "Epoch 232/300\n",
      "10/10 [==============================] - 5s - loss: 0.0338 - acc: 0.9933     \n",
      "Epoch 233/300\n",
      "10/10 [==============================] - 5s - loss: 0.0407 - acc: 0.9843     \n",
      "Epoch 234/300\n",
      "10/10 [==============================] - 4s - loss: 0.0587 - acc: 0.9850     \n",
      "Epoch 235/300\n",
      "10/10 [==============================] - 4s - loss: 0.0308 - acc: 0.9917     \n",
      "Epoch 236/300\n",
      "10/10 [==============================] - 5s - loss: 0.0166 - acc: 0.9953     \n",
      "Epoch 237/300\n",
      "10/10 [==============================] - 5s - loss: 0.0313 - acc: 0.9922     \n",
      "Epoch 238/300\n",
      "10/10 [==============================] - 5s - loss: 0.0260 - acc: 0.9906     \n",
      "Epoch 239/300\n",
      "10/10 [==============================] - 5s - loss: 0.0518 - acc: 0.9761     \n",
      "Epoch 240/300\n",
      "10/10 [==============================] - 5s - loss: 0.0303 - acc: 0.9922     \n",
      "Epoch 241/300\n",
      "10/10 [==============================] - 4s - loss: 0.0383 - acc: 0.9933     \n",
      "Epoch 242/300\n",
      "10/10 [==============================] - 5s - loss: 0.0472 - acc: 0.9828     \n",
      "Epoch 243/300\n",
      "10/10 [==============================] - 4s - loss: 0.0648 - acc: 0.9819     \n",
      "Epoch 244/300\n",
      "10/10 [==============================] - 5s - loss: 0.0325 - acc: 0.9870     \n",
      "Epoch 245/300\n",
      "10/10 [==============================] - 4s - loss: 0.0359 - acc: 0.9859     \n",
      "Epoch 246/300\n",
      "10/10 [==============================] - 4s - loss: 0.0402 - acc: 0.9859     \n",
      "Epoch 247/300\n",
      "10/10 [==============================] - 4s - loss: 0.0516 - acc: 0.9812     \n",
      "Epoch 248/300\n",
      "10/10 [==============================] - 4s - loss: 0.0214 - acc: 0.9949     \n",
      "Epoch 249/300\n",
      "10/10 [==============================] - 4s - loss: 0.0584 - acc: 0.9839     \n",
      "Epoch 250/300\n",
      "10/10 [==============================] - 4s - loss: 0.0426 - acc: 0.9859     \n",
      "Epoch 251/300\n",
      "10/10 [==============================] - 4s - loss: 0.0731 - acc: 0.9792     \n",
      "Epoch 252/300\n",
      "10/10 [==============================] - 4s - loss: 0.0271 - acc: 0.9901     \n",
      "Epoch 253/300\n",
      "10/10 [==============================] - 4s - loss: 0.0280 - acc: 0.9953     \n",
      "Epoch 254/300\n",
      "10/10 [==============================] - 5s - loss: 0.0434 - acc: 0.9937     \n",
      "Epoch 255/300\n",
      "10/10 [==============================] - 4s - loss: 0.0406 - acc: 0.9850     \n",
      "Epoch 256/300\n",
      "10/10 [==============================] - 5s - loss: 0.0344 - acc: 0.9906     \n",
      "Epoch 257/300\n",
      "10/10 [==============================] - 4s - loss: 0.0520 - acc: 0.9855     \n",
      "Epoch 258/300\n",
      "10/10 [==============================] - 4s - loss: 0.0353 - acc: 0.9882     \n",
      "Epoch 259/300\n",
      "10/10 [==============================] - 4s - loss: 0.0279 - acc: 0.9922     \n",
      "Epoch 260/300\n",
      "10/10 [==============================] - 4s - loss: 0.0702 - acc: 0.9776     \n",
      "Epoch 261/300\n",
      "10/10 [==============================] - 4s - loss: 0.0445 - acc: 0.9890     \n",
      "Epoch 262/300\n",
      "10/10 [==============================] - 4s - loss: 0.0226 - acc: 0.9913     \n",
      "Epoch 263/300\n",
      "10/10 [==============================] - 4s - loss: 0.0507 - acc: 0.9875     \n",
      "Epoch 264/300\n",
      "10/10 [==============================] - 5s - loss: 0.0365 - acc: 0.9890     \n",
      "Epoch 265/300\n",
      "10/10 [==============================] - 5s - loss: 0.0279 - acc: 0.9902     \n",
      "Epoch 266/300\n",
      "10/10 [==============================] - 4s - loss: 0.0374 - acc: 0.9882     \n",
      "Epoch 267/300\n",
      "10/10 [==============================] - 4s - loss: 0.0476 - acc: 0.9862     \n",
      "Epoch 268/300\n",
      "10/10 [==============================] - 5s - loss: 0.0490 - acc: 0.9812     \n",
      "Epoch 269/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 5s - loss: 0.0487 - acc: 0.9890     \n",
      "Epoch 270/300\n",
      "10/10 [==============================] - 4s - loss: 0.0384 - acc: 0.9859     \n",
      "Epoch 271/300\n",
      "10/10 [==============================] - 4s - loss: 0.0333 - acc: 0.9866     \n",
      "Epoch 272/300\n",
      "10/10 [==============================] - 5s - loss: 0.0418 - acc: 0.9890     \n",
      "Epoch 273/300\n",
      "10/10 [==============================] - 5s - loss: 0.0603 - acc: 0.9823     \n",
      "Epoch 274/300\n",
      "10/10 [==============================] - 5s - loss: 0.0346 - acc: 0.9890     \n",
      "Epoch 275/300\n",
      "10/10 [==============================] - 4s - loss: 0.0385 - acc: 0.9897     \n",
      "Epoch 276/300\n",
      "10/10 [==============================] - 4s - loss: 0.0466 - acc: 0.9902     \n",
      "Epoch 277/300\n",
      "10/10 [==============================] - 4s - loss: 0.0178 - acc: 0.9953     \n",
      "Epoch 278/300\n",
      "10/10 [==============================] - 5s - loss: 0.0563 - acc: 0.9803     \n",
      "Epoch 279/300\n",
      "10/10 [==============================] - 4s - loss: 0.0491 - acc: 0.9827     \n",
      "Epoch 280/300\n",
      "10/10 [==============================] - 4s - loss: 0.0428 - acc: 0.9859     \n",
      "Epoch 281/300\n",
      "10/10 [==============================] - 4s - loss: 0.0446 - acc: 0.9922     \n",
      "Epoch 282/300\n",
      "10/10 [==============================] - 4s - loss: 0.0372 - acc: 0.9917     \n",
      "Epoch 283/300\n",
      "10/10 [==============================] - 5s - loss: 0.0368 - acc: 0.9843     \n",
      "Epoch 284/300\n",
      "10/10 [==============================] - 5s - loss: 0.0301 - acc: 0.9937     \n",
      "Epoch 285/300\n",
      "10/10 [==============================] - 4s - loss: 0.0335 - acc: 0.9922     \n",
      "Epoch 286/300\n",
      "10/10 [==============================] - 5s - loss: 0.0388 - acc: 0.9875     \n",
      "Epoch 287/300\n",
      "10/10 [==============================] - 5s - loss: 0.0338 - acc: 0.9953     \n",
      "Epoch 288/300\n",
      "10/10 [==============================] - 4s - loss: 0.0388 - acc: 0.9881     \n",
      "Epoch 289/300\n",
      "10/10 [==============================] - 5s - loss: 0.0247 - acc: 0.9953     \n",
      "Epoch 290/300\n",
      "10/10 [==============================] - 4s - loss: 0.0388 - acc: 0.9917     \n",
      "Epoch 291/300\n",
      "10/10 [==============================] - 4s - loss: 0.0259 - acc: 0.9890     \n",
      "Epoch 292/300\n",
      "10/10 [==============================] - 4s - loss: 0.0246 - acc: 0.9933     \n",
      "Epoch 293/300\n",
      "10/10 [==============================] - 4s - loss: 0.0409 - acc: 0.9890     \n",
      "Epoch 294/300\n",
      "10/10 [==============================] - 4s - loss: 0.0223 - acc: 0.9906     \n",
      "Epoch 295/300\n",
      "10/10 [==============================] - 5s - loss: 0.0316 - acc: 0.9922     \n",
      "Epoch 296/300\n",
      "10/10 [==============================] - 4s - loss: 0.0388 - acc: 0.9902     \n",
      "Epoch 297/300\n",
      "10/10 [==============================] - 4s - loss: 0.0326 - acc: 0.9906     \n",
      "Epoch 298/300\n",
      "10/10 [==============================] - 4s - loss: 0.0136 - acc: 0.9906     \n",
      "Epoch 299/300\n",
      "10/10 [==============================] - 5s - loss: 0.0524 - acc: 0.9886     \n",
      "Epoch 300/300\n",
      "10/10 [==============================] - 4s - loss: 0.0325 - acc: 0.9870     \n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS,\n",
    "#    validation_data=validation_generator,\n",
    "#    validation_steps=1\n",
    "#    nb_val_samples=nb_validation_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147351"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string = model.to_json()\n",
    "open(os.path.join('inception_model.json'), 'w').write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(os.path.join('inception_model_weights.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習を可視化\n",
    "with open(\"history.pickle\", mode='wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEICAYAAACuxNj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nk/JkzJQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f25c49a2ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5be7af23852e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAACGCAYAAADQHI0rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAC6lJREFUeJzt3X+IHPd9xvH3EzmyqZMmSnUFo1+WqVJbTUrsLKpLoElJ\nLCv+QwqkbWQwkYObAzdKISkFl0BdZAL5QRsIqLWvrUhSqOXEf5QrdRAmtjGEKNEKO46louSsptZd\nA1Yix/8okSv56R8z7q0uJ+3c3t7O6b7PC5ab+c58h8992dvn5sfOyDYREVGuN7RdQEREtCtBEBFR\nuARBREThEgQREYVLEEREFC5BEBFRuL5BIOmApJckPX+J5ZL0ZUlTkp6TdEvPsj2SflS/9gyz8IiI\nGI4mewRfAXZcZvkHgS31axz4BwBJbwPuB34P2AbcL2nNYoqNiIjh6xsEtp8GzlxmlV3A11w5DLxV\n0nXA7cDjts/Yfhl4nMsHSkREtGAY5wjWAad65qfrtku1R0TEMnJV2wUASBqnOqzEtdde++4bb7yx\n5YoiIq4sR48e/antsUH6DiMIZoANPfPr67YZ4H1z2p+abwO2J4AJgE6n4263O4SyIiLKIem/B+07\njENDk8BH66uHbgVesf0T4BCwXdKa+iTx9rotIiKWkb57BJIepvrPfq2kaaorgd4IYPtB4DHgDmAK\nOAt8rF52RtIDwJF6U/tsX+6kc0REtKBvENi+s89yA5+4xLIDwIHBSouIiFHIN4sjIgqXIIiIKFyC\nICKicAmCiIjCJQgiIgqXIIiIKFyCICKicAmCiIjCJQgiIgqXIIiIKFyCICKicAmCiIjCJQgiIgqX\nIIiIKFyCICKicAmCiIjCNQoCSTsknZA0Jem+eZZ/SdKz9euHkn7es+xCz7LJYRYfERGL1+RRlauA\n/cBtwDRwRNKk7eOvr2P7Uz3rfxK4uWcTv7D9ruGVHBERw9Rkj2AbMGX7pO1XgYPArsusfyfw8DCK\ni4iIpdckCNYBp3rmp+u2XyFpE7AZeKKn+RpJXUmHJX1o4EojImJJ9D00tEC7gUdtX+hp22R7RtIN\nwBOSfmD7hd5OksaBcYCNGzcOuaSIiLicJnsEM8CGnvn1ddt8djPnsJDtmfrnSeApLj5/8Po6E7Y7\ntjtjY2MNSoqIiGFpEgRHgC2SNktaTfVh/ytX/0i6EVgDfKenbY2kq+vptcB7gONz+0ZERHv6Hhqy\nfV7SXuAQsAo4YPuYpH1A1/brobAbOGjbPd1vAh6S9BpV6Hyu92qjiIhony7+3G5fp9Nxt9ttu4yI\niCuKpKO2O4P0zTeLIyIKlyCIiChcgiAionAJgoiIwiUIIiIKlyCIiChcgiAionAJgoiIwiUIIiIK\nlyCIiChcgiAionAJgoiIwiUIIiIKlyCIiChcgiAionAJgoiIwjUKAkk7JJ2QNCXpvnmW3y3ptKRn\n69ef9izbI+lH9WvPMIuPiIjF6/uoSkmrgP3AbcA0cETS5DyPnHzE9t45fd8G3A90AANH674vD6X6\niIhYtCZ7BNuAKdsnbb8KHAR2Ndz+7cDjts/UH/6PAzsGKzUiIpZCkyBYB5zqmZ+u2+b6sKTnJD0q\nacNC+koal9SV1D19+nTD0iMiYhiGdbL434Hrbf8u1X/9X11IZ9sTtju2O2NjY0MqKSIimmgSBDPA\nhp759XXb/7P9M9vn6tl/At7dtG9ERLSrSRAcAbZI2ixpNbAbmOxdQdJ1PbM7gf+spw8B2yWtkbQG\n2F63RUTEMtH3qiHb5yXtpfoAXwUcsH1M0j6ga3sS+HNJO4HzwBng7rrvGUkPUIUJwD7bZ5bg94iI\niAHJdts1XKTT6bjb7bZdRkTEFUXSUdudQfrmm8UREYVLEEREFC5BEBFRuARBREThEgQREYVLEERE\nFC5BEBFRuARBREThEgQREYVLEEREFC5BEBFRuARBREThEgQREYVLEEREFC5BEBFRuEZBIGmHpBOS\npiTdN8/yT0s6Xj+8/luSNvUsuyDp2fo1ObdvRES0q+8TyiStAvYDtwHTwBFJk7aP96z2DNCxfVbS\nvcAXgI/Uy35h+11DrjsiIoakyR7BNmDK9knbrwIHgV29K9h+0vbZevYw1UPqIyLiCtAkCNYBp3rm\np+u2S7kH+GbP/DWSupIOS/rQADVGRMQS6ntoaCEk3QV0gPf2NG+yPSPpBuAJST+w/cKcfuPAOMDG\njRuHWVJERPTRZI9gBtjQM7++bruIpA8AnwF22j73ervtmfrnSeAp4Oa5fW1P2O7Y7oyNjS3oF4iI\niMVpEgRHgC2SNktaDewGLrr6R9LNwENUIfBST/saSVfX02uB9wC9J5kjIqJlfQ8N2T4vaS9wCFgF\nHLB9TNI+oGt7Evgi8CbgG5IAXrS9E7gJeEjSa1Sh87k5VxtFRETLZLvtGi7S6XTc7XbbLiMi4ooi\n6ajtziB9883iiIjCJQgiIgqXIIiIKFyCICKicAmCiIjCJQgiIgqXIIiIKFyCICKicAmCiIjCJQgi\nIgqXIIiIKFyCICKicAmCiIjCJQgiIgqXIIiIKFyCICKicI2CQNIOSSckTUm6b57lV0t6pF7+XUnX\n9yz7q7r9hKTbh1d6REQMQ98gkLQK2A98ENgK3Clp65zV7gFetv1bwJeAz9d9t1I94/h3gB3A39fb\ni4iIZaLJHsE2YMr2SduvAgeBXXPW2QV8tZ5+FHi/qocX7wIO2j5n+7+AqXp7ERGxTDQJgnXAqZ75\n6bpt3nVsnwdeAX6jYd+IiGjRVW0XACBpHBivZ89Jer7NepaRtcBP2y5imchYzMpYzMpYzPrtQTs2\nCYIZYEPP/Pq6bb51piVdBbwF+FnDvtieACYAJHVtd5r+AitZxmJWxmJWxmJWxmKWpO6gfZscGjoC\nbJG0WdJqqpO/k3PWmQT21NN/BDxh23X77vqqos3AFuB7gxYbERHD13ePwPZ5SXuBQ8Aq4IDtY5L2\nAV3bk8A/A/8iaQo4QxUW1Ot9HTgOnAc+YfvCEv0uERExgEbnCGw/Bjw2p+2ve6Z/CfzxJfp+Fvjs\nAmqaWMC6K13GYlbGYlbGYlbGYtbAY6HqCE5ERJQqt5iIiChca0GwmNtWrDQNxuLTko5Lek7StyRt\naqPOUeg3Fj3rfViSJa3YK0aajIWkP6nfG8ck/euoaxyVBn8jGyU9KemZ+u/kjjbqXGqSDkh66VKX\n2Kvy5XqcnpN0S6MN2x75i+qk8wvADcBq4PvA1jnr/BnwYD29G3ikjVqXyVj8IfBr9fS9JY9Fvd6b\ngaeBw0Cn7bpbfF9sAZ4B1tTzv9l23S2OxQRwbz29Ffhx23Uv0Vj8AXAL8Pwllt8BfBMQcCvw3Sbb\nbWuPYDG3rVhp+o6F7Sdtn61nD1N9H2MlavK+AHiA6n5WvxxlcSPWZCw+Duy3/TKA7ZdGXOOoNBkL\nA79eT78F+J8R1jcytp+mujLzUnYBX3PlMPBWSdf1225bQbCY21asNAu9Dcc9VIm/EvUdi3pXd4Pt\n/xhlYS1o8r54O/B2Sd+WdFjSjpFVN1pNxuJvgLskTVNd4fjJ0ZS27Ax0W59lcYuJaEbSXUAHeG/b\ntbRB0huAvwPubrmU5eIqqsND76PaS3xa0jtt/7zVqtpxJ/AV238r6fepvtf0DtuvtV3YlaCtPYKF\n3LaCObetWGka3YZD0geAzwA7bZ8bUW2j1m8s3gy8A3hK0o+pjoFOrtATxk3eF9PApO3/dXV33x9S\nBcNK02Qs7gG+DmD7O8A1VPchKk2jz5O52gqCxdy2YqXpOxaSbgYeogqBlXocGPqMhe1XbK+1fb3t\n66nOl+y0PfA9VpaxJn8j/0a1N4CktVSHik6OssgRaTIWLwLvB5B0E1UQnB5plcvDJPDR+uqhW4FX\nbP+kX6dWDg15EbetWGkajsUXgTcB36jPl79oe2drRS+RhmNRhIZjcQjYLuk4cAH4S9srbq+54Vj8\nBfCPkj5FdeL47pX4j6Okh6nCf219PuR+4I0Ath+kOj9yB9WzX84CH2u03RU4VhERsQD5ZnFEROES\nBBERhUsQREQULkEQEVG4BEFEROESBBERhUsQREQULkEQEVG4/wMG8siEiS1dnQAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f25c66bdf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('history.pickle', mode='rb') as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(history['loss'], \"-\", label=\"loss\",)\n",
    "plt.title('training')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='center right')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history['acc'], \"-\", label=\"acc\",)\n",
    "plt.ylabel('acc')\n",
    "plt.legend(loc='center right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(history['val_loss'], \"-\", label=\"val_loss\")\n",
    "plt.title('validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('val_loss')\n",
    "plt.legend(loc='center right')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history['val_acc'], \"-\", label=\"val_acc\")\n",
    "plt.ylabel('val_acc')\n",
    "plt.legend(loc='center right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
